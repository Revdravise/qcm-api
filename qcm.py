import logging
import sys
from flask import Flask, request, jsonify
from flask_cors import CORS
import openai
import os
import json
from dotenv import load_dotenv

# Charger la cl√© API OpenAI
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# V√©rifier si la cl√© API est bien charg√©e
if not OPENAI_API_KEY:
    raise ValueError("‚ùå Erreur : La cl√© API OpenAI n'est pas trouv√©e.")

# Initialisation de l'application Flask
app = Flask(__name__)
CORS(app)

# Configuration des logs
logging.basicConfig(
    level=logging.DEBUG,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler("qcm_debug.log", mode="w", encoding="utf-8"),
        logging.StreamHandler(sys.stdout),
    ],
)

@app.route("/")
def home():
    return jsonify({"message": "‚úÖ API Flask QCM op√©rationnelle !"})

@app.route("/generate-qcm", methods=["POST"])
def generate_qcm():
    """
    Endpoint pour g√©n√©rer un QCM bas√© sur un sujet donn√©.
    """
    subject = request.args.get("subject", "").strip()
    if not subject:
        return jsonify({"error": "Sujet manquant"}), 400

    logging.info(f"üîπ G√©n√©ration du QCM pour : {subject}")

    try:
        client = openai.OpenAI(api_key=OPENAI_API_KEY)

        # Construire la requ√™te pour OpenAI
        prompt = f"""
        G√©n√®re un QCM sur "{subject}" avec **exactement 5 questions**. 
        R√©ponds en JSON strict sous ce format :

        {{
            "qcm": [
                {{
                    "question": "Texte de la question",
                    "options": {{
                        "A": "Option A",
                        "B": "Option B",
                        "C": "Option C",
                        "D": "Option D",
                        "E": "Option E"
                    }},
                    "correct_answers": ["A", "C"],
                    "explanation": "Explication d√©taill√©e."
                }}
            ]
        }}

        Ne retourne que du JSON strict, sans texte additionnel.
        """

        response = client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "system", "content": "Tu es un assistant qui g√©n√®re des QCM m√©dicaux en JSON strict."},
                      {"role": "user", "content": prompt}],
            max_tokens=800,
            temperature=0.3,
        )

        # R√©cup√©rer la r√©ponse brute de l'API OpenAI
        qcm_text = response.choices[0].message.content.strip()

        # DEBUG : Afficher imm√©diatement dans le terminal et les logs
        print(f"üîé R√©ponse brute OpenAI : {qcm_text}")
        logging.info(f"‚úÖ R√©ponse brute de l'API OpenAI : {qcm_text}")

        # V√©rifier et convertir la r√©ponse en JSON
        try:
            qcm_json = json.loads(qcm_text)

            # V√©rifier si la structure est correcte
            if isinstance(qcm_json, dict) and "qcm" in qcm_json and isinstance(qcm_json["qcm"], list):
                logging.info("‚úÖ Format JSON valide re√ßu.")
                return jsonify(qcm_json)
            else:
                logging.error("‚ùå Format JSON incorrect.")
                return jsonify({"error": "Format JSON incorrect"}), 500

        except json.JSONDecodeError:
            logging.error("‚ùå OpenAI a retourn√© un JSON invalide.")
            return jsonify({"error": "L'API OpenAI a retourn√© un JSON invalide."}), 500

    except Exception as e:
        logging.error(f"‚ùå Erreur QCM : {e}")
        return jsonify({"error": str(e)}), 500

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=8002, debug=True)
